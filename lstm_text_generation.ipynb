{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from keras import backend as K\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproducibility\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] ='0'\n",
    "#Below is necessary for starting Numpy generated \n",
    "#random numbers in a well-defined initial state\n",
    "np.random.seed(42)\n",
    "\n",
    "#Below is necessary for starting core Python generated \n",
    "#random numbers in a well-defined initial state\n",
    "rn.seed(12345)\n",
    "'''force tensorflow to use single thread\n",
    "   multiple threads are the potential source\n",
    "   of non-reproducible results\n",
    "'''\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#The below tf.set_random_seed() will make random number generation in\n",
    "#Tensorflow backend have well defined initial state\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define activation functions\n",
    "def tf_sqnlsig(x):\n",
    "    u=tf.clip_by_value(x,-2,2)\n",
    "    a = u\n",
    "    b= tf.negative(tf.abs(u))\n",
    "    wsq = (tf.multiply(a,b))/4.0\n",
    "    y = tf.add(tf.multiply(tf.add(u,wsq),0.5),0.5)\n",
    "    return y\n",
    "get_custom_objects().update({'custom_activation': Activation(tf_sqnlsig)})\n",
    "def tf_sqnl(x): \n",
    "    u=tf.clip_by_value(x,-2,2)\n",
    "    a = u\n",
    "    b= tf.negative(tf.abs(u))\n",
    "    wsq = (tf.multiply(a,b))/4.0\n",
    "    y = tf.add(u,wsq)\n",
    "    return y\n",
    "get_custom_objects().update({'custom_activation': Activation(tf_sqnl)})\n",
    "\n",
    "def tf_tansig(x): \n",
    "    w=tf.multiply(tf.negative(2.0), x)\n",
    "    u=tf.exp(w)\n",
    "    a = 1.0 + u\n",
    "    b= 2.0/a\n",
    "    y = b - 1.0\n",
    "    return y\n",
    "get_custom_objects().update({'custom_activation': Activation(tf_tansig)})\n",
    "\n",
    "def tf_logsig(x): \n",
    "    u=tf.exp(tf.negative(x)) \n",
    "    a = tf.add(u,1.0)\n",
    "    y= 1.0/a\n",
    "    return y\n",
    "get_custom_objects().update({'custom_activation': Activation(tf_logsig)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n",
      "nb sequences: 200285\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "#Get Data and preprocess\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "#keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid'#keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid'\n",
    "#model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "#model.add(keras.layers.LSTM(128 , input_shape=(maxlen, len(chars)), activation=tf_tansig, recurrent_activation=tf_logsig))\n",
    "model.add(keras.layers.LSTM(128 , input_shape=(maxlen, len(chars)),  activation=tf_sqnl, recurrent_activation=tf_sqnlsig))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "200285/200285 [==============================] - 189s 944us/step - loss: 2.1537\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"uxiliaries; nevertheless, the sum of\n",
      "the\"\n",
      "uxiliaries; nevertheless, the sum of\n",
      "the person and the and the meral the proman and the greation and the person and the man and the because of the the sentice, and the something and the will the proble and the perman and the sentice and the ham the person and all the stilly in the mantient and and the which the sectance and the mandices and the sentices and the manist and in the mandices and the mentices and and the senting and the man\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"uxiliaries; nevertheless, the sum of\n",
      "the\"\n",
      "uxiliaries; nevertheless, the sum of\n",
      "the man of the prowes the lake and the juded as is the manity the geadine of the scient and incontices and and concempation in the man\n",
      "and man and surtame\n",
      "and to the contial spiritual is entimes the morality and the just is accentices and the pearands to the sartices and sentime--in the all sachine lifthine and merally to de promital to the stinction the greatine of stentiness and stally one which is\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"uxiliaries; nevertheless, the sum of\n",
      "the\"\n",
      "uxiliaries; nevertheless, the sum of\n",
      "the\n",
      "phoment and read to pechuse\n",
      "gentrmosophieto\n",
      "most (in--and ally lighte (simequed, even. it inpocfilitys--ow who the\n",
      "ear philorals at the ke\n",
      "condiitentatoy tuman,\"\n",
      "\n",
      "\n",
      "\n",
      "o2\n",
      "\n",
      "=thimab-ey is courdual plahilly found--conscivetool, a that ham the such con\n",
      "man expraestital weterly cledandally uspossed indiceen lowtion)ly\n",
      "and is iscatsing meat, in\n",
      "othas of litsed is arast knowlemsd. a thrond at the victed, w\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"uxiliaries; nevertheless, the sum of\n",
      "the\"\n",
      "uxiliaries; nevertheless, the sum of\n",
      "the relfifiedon to halkes\" whret\n",
      "indighelty conte lestige. himmish--veans in tha whichapace shentemingruienes and somteness ny\n",
      "conpleess\n",
      "difleraterstieve safeads a peinement of\n",
      "this ficte ghimoriation,\n",
      "as itsnic mude, mrse, bealt, confocricibner fay kelisteth wo meromity--ruil hits\n",
      "man, he stan-condicratly to ywe on eal\n",
      "wornd imout; in actiny of cheme obralioy in the guet?\n",
      "the\n",
      "-lelful degevictive bat\n",
      "Epoch 2/4\n",
      "200285/200285 [==============================] - 185s 921us/step - loss: 1.7342\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" in\n",
      "short, how childish and childlike th\"\n",
      " in\n",
      "short, how childish and childlike the supposed to the many and its and the sacrifice of the superselves of the precised of the sacrificed and and the sacrifice the conscious to the something and the superfoluct of the precised and the sense the sense of all the precised and the sense of the conseased and the consense and and the supposed and the present and the consention of the precustoms of the supersed and and the sense to and th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" in\n",
      "short, how childish and childlike th\"\n",
      " in\n",
      "short, how childish and childlike the conseased to be devertial that the should be order of the presense of all have the sallated for their whom in traus of the condement for the\n",
      "prepery to evil for the bad the logic to all and out the decesmotal concerces of the urone the farse, and\n",
      "experience was the sacrifice and to its the scood, that the need and regards to whomen\n",
      "\n",
      "\n",
      "\n",
      "135. the\n",
      "precusion of all order of an and that to the suppose\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" in\n",
      "short, how childish and childlike th\"\n",
      " in\n",
      "short, how childish and childlike their himself, timed, not for whand tend\"-ire intite,\n",
      "abtough instancy this some so well-wlanfers it\n",
      "is and right\n",
      "or renomirtiess dound destine\n",
      "and mankinctives of this comported, and heal\n",
      "man,\n",
      "in stand: and prests: in purcerriciabiling the whatied.\n",
      "\n",
      "12of vary indeficed thather inderfulabs\n",
      "cordined and finitaul in pare--man, less hele, that, something of dulegted that eegherst what whise\n",
      "its would h\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" in\n",
      "short, how childish and childlike th\"\n",
      " in\n",
      "short, how childish and childlike the in the such as not that accieteted\n",
      "rrinds it hat sendagie that infering that sains, neerse.\" laing\n",
      "nach\n",
      "seeliefied nhing debors to beings a danganamind\n",
      "po paed--this caney. thinkerey ham staning--by hald, he have man \"things, if\n",
      "feling through sute vorm no\n",
      "teralstuebed of to othiro, staal.\n",
      "\n",
      "133re uslavided say\n",
      "was usiblem \"barks attitignoved.--trow of\n",
      "-hi contempt sensedable rebueded one dogeble\n",
      "Epoch 3/4\n",
      "200285/200285 [==============================] - 212s 1ms/step - loss: 1.6579\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" long to visit at the most\n",
      "decisive time\"\n",
      " long to visit at the most\n",
      "decisive times the such the disideal in the most itself the something and the sight the appropesing the in the such the comparised to the say and the way and all the supposent in the consequently and in the are and"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adedamola/Tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all the say and a survicated and in the say and the supersatisment the for the power of the of the sighted and the man and something the something and the form and the for the all the say and a surv\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" long to visit at the most\n",
      "decisive time\"\n",
      " long to visit at the most\n",
      "decisive times of the formority in the possesseries of the in the bey whole man in the life in the immoraley the\n",
      "searned and spirit we have there is a sense? in the in, that the say in the in the in the inforestly, in the power to the\n",
      "cannal and man\n",
      "and desirefoly a sour musing word\n",
      "sensive of the wordinal the are have\n",
      "-the account of the approne in the all loves they themoration for the happiness the somethin\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" long to visit at the most\n",
      "decisive time\"\n",
      " long to visit at the most\n",
      "decisive times anothes theishow dangelish doinons, a griech, and whantimative moral of hart, to at in men, the sometest restore's acture for a regard and in the\n",
      "over vove reverrout him conalf\n",
      "an it, oncrepute inithing,\n",
      "that\n",
      "cannees;\n",
      "cannawnt errvictince instinarbine\n",
      "the saines,\n",
      "which his the consirpural, thery obligatenesss\" of\n",
      "yould not,\"h\n",
      "among perears man, with its supersonalism, upy wavidn.\"--of medialliau\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" long to visit at the most\n",
      "decisive time\"\n",
      " long to visit at the most\n",
      "decisive time\n",
      "itrronvation a forbeafthings lotin exision\n",
      "higher. but in the picious buty. upone prewars as this had styperablely-appo\n",
      "trace?\" musinhounty--so them a more thuts resple at otherat \"man while-ambitiatems and appearain. and he of bur inviect opedes--which\n",
      "shebtom whongever believe antearlifuted vemptime the withouth,-- wely; nature fortain,\n",
      "or to the much a will:\n",
      "-he is this upbein recalled. this t\n",
      "Epoch 4/4\n",
      "200285/200285 [==============================] - 218s 1ms/step - loss: 1.6278\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"pillar. what binds strongest? what cords\"\n",
      "pillar. what binds strongest? what cords to the stand to the stand to the conception to the still the stand the master the stand the stand to all the stand and the stand the stand the condition of the stand the stand the stand the masters to the condition of the stand to the master to all the stand the stand the more still the stand to the not and in the conception of the stand to the stard and with the stand to be the still and and and\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"pillar. what binds strongest? what cords\"\n",
      "pillar. what binds strongest? what cords the mistors to means it with world to a percessing of a person and of the world so make at all and every word and and who is the druth in the and for the purpose and that there an inder the mere and to the themselves of the properation of the all the distand condition and who the condition to the will so of latic who even conception and condition to at the word to the for the stand, the so a the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"pillar. what binds strongest? what cords\"\n",
      "pillar. what binds strongest? what cords who itropary? uprond, just that met speck,\n",
      "breat christian us: which hor to lill\"--at all sumpore even a spirit, are\n",
      "que as at this person vircip of dome, god opperine-oftred! to thy deamptly at that and to encented and general, bain oppulse, so, divide evench requill the probably as ther seement\", and is\n",
      "no one all of a all of conduct\" to all always\n",
      "ressel modenness of his syltedm, and an using \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"pillar. what binds strongest? what cords\"\n",
      "pillar. what binds strongest? what cords book--wo think if the for adond con of they were\n",
      "by takest\" ye fold, the rtend didbe\n",
      "to port\n",
      "and itow\n",
      "by his tertablely like, and must somere casess, that it has an evendem correct and ulhy\n",
      "proce, inreppreamlentmy--so ape-olth,\n",
      "that soil of it encemplity eestyling lehlave with ly, irod would i flow sensech must as the\n",
      "mare\n",
      "arbits belamatnors, not germans\"\" but\n",
      "so of beanks we always certain, to g\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a706828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=4,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
